{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_data(dataset_path):\n",
    "    X, y = [], []\n",
    "    # Iterate through splits: training, validation, testing\n",
    "    for split in [\"training\", \"validation\", \"testing\"]:\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "        if not os.path.exists(split_path):\n",
    "            print(f\"Split path does not exist: {split_path}\")\n",
    "            continue\n",
    "        # Iterate through labels: fake, real\n",
    "        for label in [\"fake\", \"real\"]:\n",
    "            label_path = os.path.join(split_path, label)\n",
    "            if not os.path.exists(label_path):\n",
    "                print(f\"Label path does not exist: {label_path}\")\n",
    "                continue\n",
    "            print(f\"Processing files in: {label_path}\")\n",
    "            # Iterate through audio files\n",
    "            for file in os.listdir(label_path):\n",
    "                file_path = os.path.join(label_path, file)\n",
    "                try:\n",
    "                    audio, sr = librosa.load(file_path, sr=16000)\n",
    "                    if len(audio) < 16000:  # Pad to 1 second if shorter\n",
    "                        audio = np.pad(audio, (0, 16000 - len(audio)), mode=\"constant\")\n",
    "                    else:\n",
    "                        audio = audio[:16000]  # Trim to 1 second\n",
    "                    mfcc = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=40)\n",
    "                    mfcc = np.mean(mfcc.T, axis=0)  # Mean across time axis\n",
    "                    X.append(mfcc)\n",
    "                    y.append(label)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {file_path}: {e}\")\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\training\\fake\n",
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\training\\real\n",
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\validation\\fake\n",
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\validation\\real\n",
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\testing\\fake\n",
      "Processing files in: C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\\testing\\real\n",
      "Loaded 17870 samples.\n"
     ]
    }
   ],
   "source": [
    "dataset_path = r\"C:\\Desktop\\Jupyter Files\\Deepfake\\for-2sec\\for-2seconds\"\n",
    "\n",
    "# Load the data\n",
    "X, y = load_audio_data(dataset_path)\n",
    "\n",
    "print(f\"Loaded {len(X)} samples.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights to handle imbalance\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(np.argmax(y, axis=1)), y=np.argmax(y, axis=1))\n",
    "class_weights = {i: class_weights[i] for i in range(len(class_weights))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Desktop\\Jupyter Files\\myenv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential([\n",
    "    Dense(256, activation=\"relu\", input_shape=(X_train.shape[1],)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "    Dense(64, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(2, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=\"adam\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks for learning rate scheduling and early stopping\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor=\"val_loss\", patience=7, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=3, min_lr=1e-6)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "358/358 - 7s - 20ms/step - accuracy: 0.7673 - loss: 0.5354 - val_accuracy: 0.9220 - val_loss: 0.2144 - learning_rate: 1.0000e-03\n",
      "Epoch 2/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.8725 - loss: 0.3081 - val_accuracy: 0.9469 - val_loss: 0.1446 - learning_rate: 1.0000e-03\n",
      "Epoch 3/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9119 - loss: 0.2265 - val_accuracy: 0.9650 - val_loss: 0.0928 - learning_rate: 1.0000e-03\n",
      "Epoch 4/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9304 - loss: 0.1836 - val_accuracy: 0.9741 - val_loss: 0.0752 - learning_rate: 1.0000e-03\n",
      "Epoch 5/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9339 - loss: 0.1701 - val_accuracy: 0.9759 - val_loss: 0.0654 - learning_rate: 1.0000e-03\n",
      "Epoch 6/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9438 - loss: 0.1482 - val_accuracy: 0.9776 - val_loss: 0.0657 - learning_rate: 1.0000e-03\n",
      "Epoch 7/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9502 - loss: 0.1292 - val_accuracy: 0.9759 - val_loss: 0.0657 - learning_rate: 1.0000e-03\n",
      "Epoch 8/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9524 - loss: 0.1252 - val_accuracy: 0.9818 - val_loss: 0.0555 - learning_rate: 1.0000e-03\n",
      "Epoch 9/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9580 - loss: 0.1129 - val_accuracy: 0.9846 - val_loss: 0.0458 - learning_rate: 1.0000e-03\n",
      "Epoch 10/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9608 - loss: 0.1030 - val_accuracy: 0.9797 - val_loss: 0.0517 - learning_rate: 1.0000e-03\n",
      "Epoch 11/100\n",
      "358/358 - 3s - 7ms/step - accuracy: 0.9586 - loss: 0.1073 - val_accuracy: 0.9857 - val_loss: 0.0383 - learning_rate: 1.0000e-03\n",
      "Epoch 12/100\n",
      "358/358 - 2s - 7ms/step - accuracy: 0.9651 - loss: 0.0971 - val_accuracy: 0.9857 - val_loss: 0.0437 - learning_rate: 1.0000e-03\n",
      "Epoch 13/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9655 - loss: 0.0933 - val_accuracy: 0.9857 - val_loss: 0.0390 - learning_rate: 1.0000e-03\n",
      "Epoch 14/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9647 - loss: 0.0937 - val_accuracy: 0.9885 - val_loss: 0.0351 - learning_rate: 1.0000e-03\n",
      "Epoch 15/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9685 - loss: 0.0861 - val_accuracy: 0.9776 - val_loss: 0.0548 - learning_rate: 1.0000e-03\n",
      "Epoch 16/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9711 - loss: 0.0838 - val_accuracy: 0.9818 - val_loss: 0.0428 - learning_rate: 1.0000e-03\n",
      "Epoch 17/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9701 - loss: 0.0786 - val_accuracy: 0.9864 - val_loss: 0.0383 - learning_rate: 1.0000e-03\n",
      "Epoch 18/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9749 - loss: 0.0722 - val_accuracy: 0.9846 - val_loss: 0.0386 - learning_rate: 5.0000e-04\n",
      "Epoch 19/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9729 - loss: 0.0722 - val_accuracy: 0.9860 - val_loss: 0.0365 - learning_rate: 5.0000e-04\n",
      "Epoch 20/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9757 - loss: 0.0680 - val_accuracy: 0.9853 - val_loss: 0.0390 - learning_rate: 5.0000e-04\n",
      "Epoch 21/100\n",
      "358/358 - 2s - 6ms/step - accuracy: 0.9783 - loss: 0.0603 - val_accuracy: 0.9867 - val_loss: 0.0369 - learning_rate: 2.5000e-04\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.04830438643693924, Test Accuracy: 0.9857302904129028\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m112/112\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "Confusion Matrix:\n",
      " [[1772   15]\n",
      " [  36 1751]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.99      1787\n",
      "           1       0.99      0.98      0.99      1787\n",
      "\n",
      "    accuracy                           0.99      3574\n",
      "   macro avg       0.99      0.99      0.99      3574\n",
      "weighted avg       0.99      0.99      0.99      3574\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate predictions\n",
    "y_pred = np.argmax(model.predict(X_test), axis=1)\n",
    "\n",
    "# Convert y_test if it's one-hot encoded\n",
    "if len(y_test.shape) > 1:\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(\"\\nClassification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model and label encoder\n",
    "model.save(\"fake_or_real_audio_lstm_model.h5\")\n",
    "import pickle\n",
    "with open(\"label_encoder.pkl\", \"wb\") as f:\n",
    "    pickle.dump(label_encoder, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
